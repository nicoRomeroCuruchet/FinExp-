{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def imprimir_encabezados_reales():\n",
    "    archivo = \"capIQReport.xls\" # <--- Aseg√∫rate que se llame as√≠\n",
    "    \n",
    "    print(f\"üîç Escaneando {archivo}...\")\n",
    "    \n",
    "    try:\n",
    "        # Leemos las primeras 15 filas sin asurmir cu√°l es el encabezado\n",
    "        df = pd.read_excel(archivo, header=None, nrows=15)\n",
    "        \n",
    "        encontrado = False\n",
    "        \n",
    "        # Buscamos fila por fila\n",
    "        for index, row in df.iterrows():\n",
    "            # Convertimos toda la fila a texto para buscar palabras clave\n",
    "            texto_fila = str(row.values)\n",
    "            \n",
    "            # Si la fila tiene \"Ticker\" y \"EPS\", esa es la cabecera\n",
    "            if \"Ticker\" in texto_fila and (\"EPS\" in texto_fila or \"Debt\" in texto_fila):\n",
    "                print(f\"\\n‚úÖ ¬°ENCABEZADOS ENCONTRADOS EN FILA {index}!\")\n",
    "                print(\"üëá COPIA Y PEGA LO SIGUIENTE EN EL CHAT üëá\\n\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                # Imprimimos cada columna limpia\n",
    "                for col in row:\n",
    "                    # Solo imprimimos si no est√° vac√≠o y parece un dato relevante\n",
    "                    if str(col) != 'nan' and (\"EPS\" in str(col) or \"Debt\" in str(col)):\n",
    "                        print(f\"COLUMNA: {col}\")\n",
    "                        \n",
    "                print(\"-\" * 60)\n",
    "                encontrado = True\n",
    "                break\n",
    "        \n",
    "        if not encontrado:\n",
    "            print(\"‚ùå No encontr√© ninguna fila que diga 'Ticker' y 'EPS' en las primeras 15 filas.\")\n",
    "            print(\"Aqu√≠ est√°n las primeras filas para que veas qu√© hay:\")\n",
    "            print(df.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ERROR: No encuentro el archivo 'capIQReport.xls' en esta carpeta.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    imprimir_encabezados_reales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. üßπ Leyendo archivo: capIQReport.xls...\n",
      "   -> Cabecera encontrada en la fila 8\n",
      "2. üßÆ Calculando fechas basadas en columnas relativas...\n",
      "   -> 960 registros procesados.\n",
      "========================================\n",
      "‚úÖ ¬°LISTO! Archivo generado: tus_datos_capiq.xlsx\n",
      "   Rango de Fechas: 2022-03-31 a 2025-11-30\n",
      "========================================\n",
      "metric       date   tic  eps_norm  total_debt\n",
      "0      2022-03-31  AAPL      4.37    122798.0\n",
      "30     2022-06-30  AAPL      4.48    119981.0\n",
      "60     2022-09-30  AAPL      4.47    119691.0\n",
      "90     2022-12-31  AAPL      4.56    132480.0\n",
      "120    2023-03-31  AAPL      4.38    111110.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def limpiar_capiq_relativo():\n",
    "    input_file = \"capIQReport.xls\"\n",
    "    output_file = \"tus_datos_capiq.xlsx\"\n",
    "    \n",
    "    # --- CONFIGURACI√ìN CLAVE ---\n",
    "    # Pon aqu√≠ la fecha APROXIMADA en que descargaste el Excel.\n",
    "    # El script asumir√° que la columna [LTM] corresponde a este trimestre.\n",
    "    FECHA_REFERENCIA = \"2025-11-28\" \n",
    "    \n",
    "    print(f\"1. üßπ Leyendo archivo: {input_file}...\")\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ùå ERROR: No encuentro '{input_file}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Buscar la cabecera din√°micamente\n",
    "        df_raw = pd.read_excel(input_file, header=None)\n",
    "        start_row = 0\n",
    "        for i, row in df_raw.iterrows():\n",
    "            row_str = row.astype(str).str.cat()\n",
    "            if \"Ticker\" in row_str and (\"EPS\" in row_str or \"Debt\" in row_str):\n",
    "                start_row = i\n",
    "                break\n",
    "        \n",
    "        print(f\"   -> Cabecera encontrada en la fila {start_row + 1}\")\n",
    "        df = pd.read_excel(input_file, header=start_row)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error leyendo Excel: {e}\")\n",
    "        return\n",
    "\n",
    "    col_ticker = next((c for c in df.columns if \"Ticker\" in str(c)), None)\n",
    "    if not col_ticker:\n",
    "        print(\"‚ùå Error: No encuentro columna Ticker.\")\n",
    "        return\n",
    "\n",
    "    print(\"2. üßÆ Calculando fechas basadas en columnas relativas...\")\n",
    "    \n",
    "    # Convertimos la fecha de referencia a objeto fecha\n",
    "    ref_date = pd.to_datetime(FECHA_REFERENCIA)\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    # Regex para capturar el numero de offset: \"LTM - 15\" o \"Latest Quarter - 3\"\n",
    "    # Grupo 1: El Offset (si existe)\n",
    "    regex_offset = re.compile(r'\\[(?:LTM|Latest Quarter)(?: - (\\d+))?\\]')\n",
    "\n",
    "    # Convertir a formato largo\n",
    "    df_melt = df.melt(id_vars=[col_ticker], var_name='raw_col', value_name='value')\n",
    "    # Limpiar valores no num√©ricos\n",
    "    df_melt['value'] = pd.to_numeric(df_melt['value'], errors='coerce')\n",
    "    df_melt = df_melt.dropna(subset=['value'])\n",
    "\n",
    "    count = 0\n",
    "    for idx, row in df_melt.iterrows():\n",
    "        header = str(row['raw_col'])\n",
    "        \n",
    "        # 1. Identificar M√©trica\n",
    "        metric = None\n",
    "        if \"Normalized Diluted EPS\" in header: metric = \"eps_norm\"\n",
    "        elif \"Total Debt\" in header: metric = \"total_debt\"\n",
    "        \n",
    "        if not metric: continue\n",
    "\n",
    "        # 2. Calcular Fecha\n",
    "        match = regex_offset.search(header)\n",
    "        if match:\n",
    "            # Si captura un numero (ej: 15), es el offset. Si es None, es 0 (LTM actual)\n",
    "            offset_quarters = int(match.group(1)) if match.group(1) else 0\n",
    "            \n",
    "            # CALCULO DE FECHA: Fecha Ref - (Offset * 3 meses)\n",
    "            # Usamos 91 dias como aprox de un trimestre para ser r√°pidos\n",
    "            days_to_subtract = offset_quarters * 91 \n",
    "            calculated_date = ref_date - timedelta(days=days_to_subtract)\n",
    "            \n",
    "            processed_data.append({\n",
    "                'tic': str(row[col_ticker]).replace('NYSE:', '').replace('NasdaqGS:', ''),\n",
    "                'date': calculated_date,\n",
    "                'metric': metric,\n",
    "                'value': row['value']\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "    if not processed_data:\n",
    "        print(\"‚ùå ERROR: No se pudieron procesar datos. Revisa el regex.\")\n",
    "        return\n",
    "\n",
    "    print(f\"   -> {count} registros procesados.\")\n",
    "\n",
    "    # Paso 3: Armar tabla final\n",
    "    df_clean = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Normalizar fechas al fin de mes m√°s cercano (opcional, para que quede bonito)\n",
    "    df_clean['date'] = df_clean['date'] + pd.offsets.MonthEnd(0)\n",
    "    \n",
    "    # Pivotar\n",
    "    df_final = df_clean.pivot_table(index=['date', 'tic'], columns='metric', values='value').reset_index()\n",
    "    df_final = df_final.sort_values(['tic', 'date'])\n",
    "    \n",
    "    df_final.to_excel(output_file, index=False)\n",
    "    print(\"=\"*40)\n",
    "    print(f\"‚úÖ ¬°LISTO! Archivo generado: {output_file}\")\n",
    "    print(f\"   Rango de Fechas: {df_final.date.min().date()} a {df_final.date.max().date()}\")\n",
    "    print(\"=\"*40)\n",
    "    print(df_final.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    limpiar_capiq_relativo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. üìâ Descargando Precios + Macro de Yahoo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (43457, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (4526, 8)\n",
      "2. üìä Calculando Indicadores T√©cnicos...\n",
      "Successfully added technical indicators\n",
      "3. üîó Fusionando con Datos Fundamentales (Capital IQ)...\n",
      "   ‚úÖ ¬°Datos de Capital IQ integrados y P/E calculado!\n",
      "4. üåé Uniendo Macro a cada Acci√≥n...\n",
      "   -> Generando matrices de covarianza (esto toma unos segundos)...\n",
      "5. üèÅ DataFrame Final Listo. Shape: (35196, 17)\n",
      "\n",
      "--- MUESTRA DE DATOS (Verificaci√≥n) ---\n",
      "           date  tic       close  eps_norm  pe_ratio_daily    vix\n",
      "1256 2023-12-29  TRV  184.812943      8.16       22.620923  12.45\n",
      "1256 2023-12-29  UNH  509.297058     17.30       29.422129  12.45\n",
      "1256 2023-12-29    V  256.552917      6.11       41.920411  12.45\n",
      "1256 2023-12-29   VZ   33.068073      3.92        8.414268  12.45\n",
      "1256 2023-12-29  WMT   51.540096      1.81       28.318734  12.45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
    "from finrl import config_tickers\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNCI√ìN DE COVARIANZA (Necesaria para el Portfolio Env)\n",
    "# ==============================================================================\n",
    "def add_covariance_matrix(df, lookback=252):\n",
    "    # Aseguramos que date sea columna y no indice\n",
    "    if 'date' not in df.columns:\n",
    "        df = df.reset_index()\n",
    "        \n",
    "    df = df.sort_values(['date','tic'], ignore_index=True)\n",
    "    df.index = df.date.factorize()[0]\n",
    "\n",
    "    cov_list = []\n",
    "    dates_with_cov = [] \n",
    "    \n",
    "    unique_dates = df.date.unique()\n",
    "\n",
    "    # Si no hay suficientes datos, devolvemos tal cual para evitar crash\n",
    "    if len(unique_dates) < lookback:\n",
    "        return df\n",
    "\n",
    "    print(\"   -> Generando matrices de covarianza (esto toma unos segundos)...\")\n",
    "    for i in range(lookback, len(unique_dates)):\n",
    "        current_date = unique_dates[i]\n",
    "        data_lookback = df.loc[i-lookback:i, :]\n",
    "        price_lookback = data_lookback.pivot_table(index='date', columns='tic', values='close')\n",
    "        return_lookback = price_lookback.pct_change().dropna()\n",
    "        covs = return_lookback.cov().values \n",
    "        cov_list.append(covs)\n",
    "        dates_with_cov.append(current_date)\n",
    "\n",
    "    df_cov = df[df.date.isin(dates_with_cov)].copy()\n",
    "    cov_dict = dict(zip(dates_with_cov, cov_list))\n",
    "    df_cov['cov_list'] = df_cov['date'].map(cov_dict)\n",
    "    \n",
    "    return df_cov.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNCI√ìN PRINCIPAL DE FUSI√ìN DE DATOS\n",
    "# ==============================================================================\n",
    "def prepare_custom_data():\n",
    "    print(\"1. üìâ Descargando Precios + Macro de Yahoo...\")\n",
    "    \n",
    "    dow_tickers = [t for t in config_tickers.DOW_30_TICKER if t != 'WBA']\n",
    "    \n",
    "    # A. PRECIOS (Stocks)\n",
    "    # Bajamos desde 2018 para tener suficiente historia para el lookback\n",
    "    df_prices = YahooDownloader(start_date='2018-01-01', \n",
    "                                end_date='2024-01-01', \n",
    "                                ticker_list=dow_tickers).fetch_data()\n",
    "    df_prices = df_prices.reset_index(drop=True)\n",
    "    df_prices['date'] = pd.to_datetime(df_prices['date'])\n",
    "\n",
    "    # B. MACRO (VIX, Bonos, Oro)\n",
    "    macro_tickers = ['^VIX', '^TNX', 'GC=F']\n",
    "    df_macro = YahooDownloader(start_date='2018-01-01', \n",
    "                               end_date='2024-01-01', \n",
    "                               ticker_list=macro_tickers).fetch_data()\n",
    "    \n",
    "    # Procesar Macro: Pivotar para tener columnas limpias\n",
    "    df_macro = df_macro.pivot(index='date', columns='tic', values='close').reset_index()\n",
    "    df_macro['date'] = pd.to_datetime(df_macro['date'])\n",
    "    \n",
    "    # Renombrar columnas para que sean f√°ciles de leer\n",
    "    rename_map = {'^VIX': 'vix', '^TNX': 'us_10y', 'GC=F': 'gold'}\n",
    "    df_macro.rename(columns=rename_map, inplace=True)\n",
    "    df_macro = df_macro.ffill().bfill() # Rellenar huecos de festivos\n",
    "\n",
    "    print(\"2. üìä Calculando Indicadores T√©cnicos...\")\n",
    "    fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                         tech_indicator_list=['macd', 'rsi_30'], \n",
    "                         use_turbulence=False,\n",
    "                         user_defined_feature=False)\n",
    "    df_prices = fe.preprocess_data(df_prices)\n",
    "    if 'date' not in df_prices.columns: df_prices = df_prices.reset_index()\n",
    "\n",
    "    print(\"3. üîó Fusionando con Datos Fundamentales (Capital IQ)...\")\n",
    "    try:\n",
    "        # Cargar tu Excel limpio\n",
    "        df_capiq = pd.read_excel(\"tus_datos_capiq.xlsx\")\n",
    "        df_capiq['date'] = pd.to_datetime(df_capiq['date'])\n",
    "        \n",
    "        # Verificar que existen las columnas necesarias\n",
    "        if 'tic' in df_capiq.columns and 'date' in df_capiq.columns:\n",
    "            # MERGE: Unimos precios diarios con datos trimestrales\n",
    "            # Usamos 'on' date y tic. Solo coincidir√°n las fechas de cierre de trimestre.\n",
    "            df_prices = df_prices.merge(df_capiq, on=['date', 'tic'], how='left')\n",
    "            \n",
    "            # FORWARD FILL: Aqu√≠ ocurre la magia.\n",
    "            # Rellenamos los NaNs hacia abajo usando el √∫ltimo dato conocido por Ticker.\n",
    "            fund_cols = ['eps_norm', 'total_debt']\n",
    "            df_prices[fund_cols] = df_prices.groupby('tic')[fund_cols].ffill()\n",
    "            \n",
    "            # Si al principio hay NaNs (antes del primer reporte), rellenar con 0\n",
    "            df_prices[fund_cols] = df_prices[fund_cols].fillna(0)\n",
    "\n",
    "            # CALCULAR EL \"SUPER INDICADOR\": P/E RATIO DIARIO\n",
    "            # (Sumamos 0.01 para evitar divisi√≥n por cero)\n",
    "            df_prices['pe_ratio_daily'] = df_prices['close'] / (df_prices['eps_norm'] + 0.01)\n",
    "            \n",
    "            print(\"   ‚úÖ ¬°Datos de Capital IQ integrados y P/E calculado!\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è ERROR: El Excel no tiene columnas 'date' o 'tic'. Revisa el limpiador.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ö†Ô∏è AVISO: No se encontr√≥ 'tus_datos_capiq.xlsx'.\")\n",
    "\n",
    "    print(\"4. üåé Uniendo Macro a cada Acci√≥n...\")\n",
    "    # Unimos el VIX y Bonos a cada fila\n",
    "    df_final = df_prices.merge(df_macro, on='date', how='left')\n",
    "    \n",
    "    # Limpieza final\n",
    "    df_final = df_final.sort_values(['date', 'tic']).ffill().dropna()\n",
    "    \n",
    "    # Generar Covarianzas\n",
    "    df_final = add_covariance_matrix(df_final, lookback=252)\n",
    "    \n",
    "    # Indexar num√©ricamente (Requisito de FinRL)\n",
    "    df_final.index = df_final.date.factorize()[0]\n",
    "    \n",
    "    print(f\"5. üèÅ DataFrame Final Listo. Shape: {df_final.shape}\")\n",
    "    return df_final, len(dow_tickers)\n",
    "\n",
    "# --- PRUEBA R√ÅPIDA ---\n",
    "if __name__ == \"__main__\":\n",
    "    df_train, stock_dim = prepare_custom_data()\n",
    "    \n",
    "    print(\"\\n--- MUESTRA DE DATOS (Verificaci√≥n) ---\")\n",
    "    cols_a_ver = ['date', 'tic', 'close', 'eps_norm', 'pe_ratio_daily', 'vix']\n",
    "    # Mostrar solo columnas que existan\n",
    "    print(df_train[[c for c in cols_a_ver if c in df_train.columns]].tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl-ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
